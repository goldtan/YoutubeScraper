{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62BlBTDsIBJb"
   },
   "source": [
    "# 1. 필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nZ6HPtfZnpU4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kiwipiepy==0.16.0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: kiwipiepy-model~=0.16 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from kiwipiepy==0.16.0) (0.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from kiwipiepy==0.16.0) (1.24.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from kiwipiepy==0.16.0) (4.63.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from tqdm->kiwipiepy==0.16.0) (0.4.6)\n",
      "Requirement already satisfied: youtube-transcript-api in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (0.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from youtube-transcript-api) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (2023.5.7)\n",
      "Requirement already satisfied: googletrans==3.1.0a0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.5.7)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.4.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
      "Requirement already satisfied: pyvis in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from pyvis) (8.12.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from pyvis) (3.0.1)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from pyvis) (3.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.17.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=5.3.0->pyvis) (1.15.0)\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\minchankim\\anaconda3\\lib\\site-packages (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'sudo'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!pip install kiwipiepy==0.16.0\n",
    "!pip install youtube-transcript-api\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install pyvis\n",
    "!pip install xlsxwriter\n",
    "\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "from googletrans import Translator\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "from kiwipiepy.utils import Stopwords\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi #https://pypi.org/project/youtube-transcript-api/\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvis.network as net\n",
    "\n",
    "import ast\n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "Stopwords = Stopwords()\n",
    "kiwi = Kiwi(model_type = 'sbg')\n",
    "\n",
    "def extract_script(videoID): #videoID로부터 스크립트 추출\n",
    "    try:\n",
    "        script = YouTubeTranscriptApi.get_transcript(videoID,languages=['ko'])\n",
    "        return [\"[\"+str(timedelta(seconds=comment['start'])).split('.')[0]+\"] \"+ comment['text'] for comment in script]\n",
    "    except :\n",
    "        pass\n",
    "\n",
    "def extract_kw(sentence, keyword):\n",
    "  result = [token.form for token in kiwi.tokenize(sentence, normalize_coda = True, stopwords=Stopwords) if token.tag in ['NNG', 'NNP', 'NNB', 'NR', 'NP']]\n",
    "\n",
    "  for w in keyword.split(','):\n",
    "    kw = w.strip()\n",
    "    if kw in sentence:\n",
    "      result.append(kw)\n",
    "  return list(set(result))\n",
    "\n",
    "def extract_comments(videoID): #videoID로부터 댓글 추출\n",
    "    comments=[]\n",
    "    try:\n",
    "        comment_thread_response = api_obj.commentThreads().list(part=\"snippet,replies\", videoId=videoID, maxResults=100).execute()\n",
    "    except :\n",
    "        return ['댓글이 사용중지되었습니다.'] #댓글이 사용중지되어 댓글이 없는 경우와 댓글을 달 수 있지만 사람들이 달지 않아 댓글이 0개인 상황을 구분\n",
    "    if len(comment_thread_response['items'])>0:\n",
    "        while comment_thread_response: #100개씩 밖에 크롤링할 수 없기 때문에 100개를 크롤링 한 후에 다음 페이지가 있으면 다음 페이지로 넘어감\n",
    "            for comment in comment_thread_response['items']:\n",
    "                name=comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "                text=comment['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "                #if all(keyword in text for keyword in keyword.split()): #모든 키워드가 포함되어야만\n",
    "                #if any(keyword.replace(',','') in text for keyword in keyword.split()): #하나의 키워드라도 포함된 댓글\n",
    "                if True:\n",
    "                  comments.append([name,text])\n",
    "                try: #답글의 개수가 0개 보다 많을 때 크롤링하도록 하였는데 종종 답글이 1개 이상인데 답글이 사라져 있는 경우가 있어서 예외 처리\n",
    "                    if comment['snippet']['totalReplyCount']>0:\n",
    "                        comment_response = api_obj.comments().list(part=\"snippet\", parentId=comment['id'], maxResults=100).execute()\n",
    "                        while comment_response: #100개씩 밖에 크롤링할 수 없기 때문에 100개를 크롤링 한 후에 다음 페이지가 있으면 다음 페이지로 넘어감\n",
    "                            for reply in comment_response['items']:\n",
    "                                name=reply['snippet']['authorDisplayName']\n",
    "                                text=reply['snippet']['textOriginal']\n",
    "                                #if all(keyword in text for keyword in keyword.split()): #모든 키워드가 포함되어야만\n",
    "                                #if any(keyword.replace(',','') in text for keyword in keyword.split()): #하나의 키워드라도 포함된 댓글\n",
    "                                if True:\n",
    "                                    comments.append([name,text])\n",
    "                            if 'nextPageToken' in comment_response:\n",
    "                                comment_response = api_obj.comments().list(part='snippet', parentId=comment['id'], pageToken=comment_response['nextPageToken'], maxResults=100).execute()\n",
    "                            else:\n",
    "                                break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if 'nextPageToken' in comment_thread_response:\n",
    "                comment_thread_response = api_obj.commentThreads().list(part='snippet,replies', videoId=videoID, pageToken=comment_thread_response['nextPageToken'], maxResults=100).execute()\n",
    "            else:\n",
    "                break\n",
    "    return comments\n",
    "\n",
    "def check_keyword(keywords, target_list):\n",
    "  if '|' in keywords:\n",
    "    keyword_list = [string.strip().replace('\"','') for string in keywords.split('|')]\n",
    "    for keyword in keyword_list:\n",
    "      if keyword in target_list:\n",
    "        return True\n",
    "    return False\n",
    "  else:\n",
    "    if keyword in target_list:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "def extract_text(comment_list):\n",
    "  text = [comment[1] for comment in comment_list]\n",
    "  return '. '.join(text)\n",
    "\n",
    "def get_viewCount(VideoID):\n",
    "  try:\n",
    "    return int(api_obj.videos().list(part=\"id, snippet, statistics, status, topicDetails\", id = VideoID).execute()['items'][0]['statistics']['viewCount'])\n",
    "  except KeyError:\n",
    "    return 0\n",
    "\n",
    "def get_subCount(ChannelID):\n",
    "  try:\n",
    "    return int(api_obj.channels().list(part=\"id, snippet, brandingSettings, contentDetails, statistics, topicDetails\", id = ChannelID).execute()['items'][0]['statistics']['subscriberCount'])\n",
    "  except KeyError:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9bOoWxLXpUXN"
   },
   "outputs": [],
   "source": [
    "key_list = [] #API key 입력\n",
    "api_obj = build('youtube', 'v3', developerKey=key_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWLcR1CBsM4J"
   },
   "outputs": [],
   "source": [
    "keyword_list = [] #키워드 입력\n",
    "stopwords = '' #금지어 입력\n",
    "\n",
    "def get_df(keyword_list, stopwords, time):\n",
    "  temp = pd.DataFrame()\n",
    "  for keyword in keyword_list:\n",
    "    query = keyword.replace(\",\", \"|\") +' '+ ' '.join([\"-\" + word for word in stopwords.split(', ')])\n",
    "\n",
    "    time = time # relative 상대적인 시간 absolute 절대적인 시간\n",
    "\n",
    "    if time == 'relative':\n",
    "      publishedBefore = datetime.utcnow().isoformat().split(\".\")[0] + 'Z'\n",
    "      publishedAfter = (datetime.utcnow() - relativedelta(months=1)).isoformat().split(\".\")[0] + 'Z'\n",
    "    elif time == 'absolute':\n",
    "      publishedBefore = datetime(2023, 9, 18, 0).isoformat().split(\".\")[0] + 'Z'\n",
    "      publishedAfter = datetime(2023, 8, 1, 0).isoformat().split(\".\")[0] + 'Z'\n",
    "    print('수집 기간은 ' + publishedAfter + ' ~ ' + publishedBefore +'입니다.')\n",
    "\n",
    "    videos = []\n",
    "\n",
    "    while True:\n",
    "        search_response = api_obj.search().list(part='id, snippet', maxResults='50', order='date', publishedBefore=publishedBefore,\n",
    "                                                publishedAfter=publishedAfter, q=query, regionCode='KR', safeSearch='none', type='video').execute()\n",
    "        for search_result in search_response.get(\"items\", []):\n",
    "            if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "                videos.append([search_result[\"id\"][\"videoId\"], search_result['snippet']['publishedAt'], search_result['snippet']['channelTitle'], search_result['snippet']['channelId'],\n",
    "                              search_result[\"snippet\"][\"title\"], search_result[\"snippet\"][\"description\"]])\n",
    "                publishedBefore=search_result['snippet']['publishedAt']\n",
    "        if len(videos)>=50:\n",
    "          if (videos[-1] == videos[-2]) and (videos[-2] == videos[-3]):\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    #https://developers.google.com/youtube/v3/docs/search/list?hl=ko#python,-%EC%98%88-1\n",
    "\n",
    "    data = pd.DataFrame(videos, columns =['VideoID', 'Date', 'ChannelTitle', 'ChannelID', 'Title', 'Description'])\n",
    "    data = data.drop_duplicates(['VideoID']).reset_index(drop=True)\n",
    "\n",
    "    #영상에 대한 정보\n",
    "    data['URL']= data['VideoID'].map(lambda x: 'https://www.youtube.com/watch?v='+x)\n",
    "    #data['Script']=data['VideoID'].map(lambda x: extract_script(x))\n",
    "    data['viewCount'] = data['VideoID'].map(get_viewCount) #KeyError\n",
    "\n",
    "    #채널에 대한 정보\n",
    "    data['subscriberCount'] = data['ChannelID'].map(get_subCount)\n",
    "\n",
    "    data = data[data['viewCount'] < 10000] #일정 조회 수 이하의 영상만 추출\n",
    "    data = data.loc[data['subscriberCount'] < 1000] #일정 구독자 수 이하의 영상만 추출\n",
    "\n",
    "    data['source'] = [str(row1) + ' ' + str(row2) for row1, row2 in zip(data['Title'], data['Description'])]\n",
    "    data['keyword'] = data['source'].map(lambda x : extract_kw(x, keyword))\n",
    "\n",
    "    data['keyword_en'] = data['keyword'].map(lambda x : [i.text for i in translator.translate(x)])\n",
    "\n",
    "    #data = data.loc[data['keyword'].apply(lambda x: check_keyword(keyword, x))] #영상의 키워드 중 검색한 키워드와 완벽하게 일치하는 단어가 있는 영상만 추출\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    print('수집한 영상들의 조회 수의 총합은 '+ str(sum(data['viewCount'])) + '입니다.')\n",
    "\n",
    "    data['Comments']=data['VideoID'].map(lambda x: extract_comments(x))\n",
    "    #data['Ad'] = 'Y' 미구현\n",
    "    print(data)\n",
    "\n",
    "    data['commentsKeyword'] = data['Comments'].map(lambda x: extract_text(x))\n",
    "\n",
    "    data['commentsKeyword'] = data['commentsKeyword'].map(lambda x : list(set(extract_kw(x, keyword))))\n",
    "    data['commentsKeyword_en'] = data['commentsKeyword'].map(lambda x : [i.text for i in translator.translate(x)])\n",
    "\n",
    "    data.to_excel(str(datetime.now()).split()[0]+\"_\"+keyword+'.xlsx') #20XX-00-00_keyword.xlsx 형태로 저장\n",
    "    #data.to_excel('파일명.xlsx')를 통해 원하는 이름으로 저장 가능\n",
    "\n",
    "    for w in keyword.split(','):\n",
    "      # 특정 컬럼에서 특정 단어를 포함하는 행을 찾음\n",
    "      mask = data['keyword'].apply(lambda x: w.strip() in x)\n",
    "\n",
    "      # mask를 사용하여 DataFrame에서 해당 행을 선택함\n",
    "      result = data[mask]\n",
    "\n",
    "      if len(result) > 0:\n",
    "        result.to_excel(str(datetime.now()).split()[0]+\"_\"+w.strip()+'.xlsx')\n",
    "\n",
    "    if len(temp) == 0:\n",
    "      temp = data.copy()\n",
    "    else:\n",
    "      temp = pd.concat([temp, data], axis=0)\n",
    "\n",
    "  return temp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6eO8e8M3WoT"
   },
   "outputs": [],
   "source": [
    "data = get_df(keyword_list, stopwords, time = 'absolute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHQda9fJ5BS0"
   },
   "outputs": [],
   "source": [
    "#data = data[~data['source'].str.contains('산행')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ykx9ew7WXIH"
   },
   "outputs": [],
   "source": [
    "data_1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEWjw2x_7WrI"
   },
   "outputs": [],
   "source": [
    "data.subscriberCount.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XffB7Ax5coHN"
   },
   "outputs": [],
   "source": [
    "def filter_non_korean_comments(comments_list):\n",
    "    filtered_list = []\n",
    "    print(len(comments_list))\n",
    "    if type(comments_list) == str:\n",
    "      return comments_list\n",
    "\n",
    "    for comment in comments_list:\n",
    "        author, content = comment\n",
    "        if all(ord(c) < 128 for c in content):\n",
    "            # 댓글 내용이 한글이 아닌 경우 (ASCII 범위를 벗어남)\n",
    "            continue\n",
    "        filtered_list.append(comment)\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "filtered_list = filter_non_korean_comments(comments_list)\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cP1Wurgn8rHv"
   },
   "outputs": [],
   "source": [
    "for keyword in keyword_list:\n",
    "  for w in keyword.split(','):\n",
    "        a = w.strip().replace('\"','')\n",
    "        # 특정 컬럼에서 특정 단어를 포함하는 행을 찾음\n",
    "        mask = data['keyword'].apply(lambda x: a in x)\n",
    "        print(a)\n",
    "\n",
    "        # mask를 사용하여 DataFrame에서 해당 행을 선택함\n",
    "        result = data[mask]\n",
    "\n",
    "        if len(result) > 0:\n",
    "          result.to_excel(str(datetime.now()).split()[0]+\"_\"+a+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ1bGQWPLc-L"
   },
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5yvjLYPIRKR"
   },
   "source": [
    "# 특정 단어 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHf04HfHEj2R"
   },
   "outputs": [],
   "source": [
    "# 특정 단어 리스트\n",
    "word_list = ['']\n",
    "\n",
    "# 특정 컬럼(column_name)에 특정 단어가 포함되어 있는지 확인하는 조건식\n",
    "condition = data['source'].map(str).str.contains('|'.join(word_list), case=False, na=False)\n",
    "\n",
    "# 조건을 만족하는 행들을 필터링하여 새로운 데이터프레임 생성\n",
    "filtered_df = data[condition]\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUMBUCOHr7Ce"
   },
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lENLB4No-Era"
   },
   "source": [
    "# 영상 키워드 네트워크 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGXcPI1YlgyB"
   },
   "outputs": [],
   "source": [
    "#%% class & function definition\n",
    "class Make_Network:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def document_keywords(self):\n",
    "        \"\"\"wikipedia redirection 완료된 keyphrase를 대문자 -> 소문자로 전처리 후 문서에 할당 \"\"\"\n",
    "        document_keywords = []\n",
    "        for keyword_list in self.data:\n",
    "            a = []\n",
    "            for keyword in keyword_list:\n",
    "              if len(keyword) >1: #임시\n",
    "                a.append(keyword.lower())\n",
    "            document_keywords.append(a)\n",
    "        return document_keywords\n",
    "\n",
    "    def keyword_corpus(self, frequency):\n",
    "        \"\"\" 전체 키워드 corpus 및 테이블 작성\"\"\"\n",
    "        document_keywords = self.document_keywords()\n",
    "\n",
    "        result = [] # 전체 keyphrase를 하나의 리스트로 저장\n",
    "        for keyword_list in document_keywords:\n",
    "            for keyword in keyword_list:\n",
    "                result.append(keyword)\n",
    "\n",
    "        b= Counter(result)\n",
    "        c = sorted(b.items(), key=lambda x: x[1], reverse=True) # value값으로 내림차순\n",
    "        d = pd.DataFrame(c,columns = [\"KEYWORD\",\"FREQUENCY\"])\n",
    "        keyword_corpus = d[d[\"FREQUENCY\"] >= frequency] #빈도수 k 이상의 keyphrase만 사용\n",
    "        return keyword_corpus\n",
    "\n",
    "    def keyword_adj_matrix(self, frequency):\n",
    "        \"\"\" Document keyword matrix 구성 후 내적을 통해 keyword adjacency matrix를 구축 \"\"\"\n",
    "        document_keywords = self.document_keywords()\n",
    "        keyword_corpus = self.keyword_corpus(frequency)[\"KEYWORD\"]\n",
    "\n",
    "        total_result = []\n",
    "        for document in document_keywords:\n",
    "            result = []\n",
    "            for keyword in keyword_corpus:\n",
    "                if keyword in document: #document_keywords의 keyword_corpus에 해당되는 keyphrase가 있으면 1 아니면 0\n",
    "                    result.append(1)\n",
    "                else:\n",
    "                    result.append(0)\n",
    "            total_result.append(result)\n",
    "        matrix = pd.DataFrame(total_result,columns= keyword_corpus).to_numpy()\n",
    "        keyword_matrix = np.dot(matrix.T,matrix)\n",
    "        np.fill_diagonal(keyword_matrix,0,wrap =True) # 대각 성분 0으로 변환\n",
    "        keyword_adj_matrix = pd.DataFrame(keyword_matrix, columns = keyword_corpus, index = keyword_corpus)\n",
    "        return keyword_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hQNm5NBMTXl"
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph(Make_Network(data['keyword']).keyword_adj_matrix(20))\n",
    "\n",
    "layout = nx.kamada_kawai_layout(graph)\n",
    "#layout = nx.random_layout(graph)\n",
    "#layout = nx.spectral_layout(graph)\n",
    "#layout = nx.spring_layout(graph)\n",
    "#layout = nx.planar_layout(graph)\n",
    "#layout = nx.shell_layout(graph)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "nx.draw_networkx_nodes(graph, layout, alpha = 0.7)\n",
    "nx.draw_networkx_labels(graph, layout, font_family='NanumBarunGothic', font_size = 14)\n",
    "nx.draw_networkx_edges(graph, layout, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CW3orqTnxNZF"
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph(Make_Network(data['keyword_en']).keyword_adj_matrix(20))\n",
    "\n",
    "layout = nx.kamada_kawai_layout(graph)\n",
    "#layout = nx.random_layout(graph)\n",
    "#layout = nx.spectral_layout(graph)\n",
    "#layout = nx.spring_layout(graph)\n",
    "#layout = nx.planar_layout(graph)\n",
    "#layout = nx.shell_layout(graph)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "nx.draw_networkx_nodes(graph, layout, alpha = 0.7)\n",
    "nx.draw_networkx_labels(graph, layout, font_family='NanumBarunGothic', font_size = 14)\n",
    "nx.draw_networkx_edges(graph, layout, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXlxjZW1XHaV"
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph, str(datetime.now()).split()[0]+\"_\"+keyword+'.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7BrP7MRxhTo"
   },
   "source": [
    "# 댓글 키워드 네트워크 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCrJHmqkxhTp"
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph(Make_Network(data['commentsKeyword']).keyword_adj_matrix(10))\n",
    "\n",
    "layout = nx.kamada_kawai_layout(graph)\n",
    "#layout = nx.random_layout(graph)\n",
    "#layout = nx.spectral_layout(graph)\n",
    "#layout = nx.spring_layout(graph)\n",
    "#layout = nx.planar_layout(graph)\n",
    "#layout = nx.shell_layout(graph)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "nx.draw_networkx_nodes(graph, layout, alpha = 0.7)\n",
    "nx.draw_networkx_labels(graph, layout, font_family='NanumBarunGothic', font_size = 14)\n",
    "nx.draw_networkx_edges(graph, layout, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8uSDGa_xhTp"
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph(Make_Network(data['commentsKeyword_en']).keyword_adj_matrix(10))\n",
    "\n",
    "layout = nx.kamada_kawai_layout(graph)\n",
    "#layout = nx.random_layout(graph)\n",
    "#layout = nx.spectral_layout(graph)\n",
    "#layout = nx.spring_layout(graph)\n",
    "#layout = nx.planar_layout(graph)\n",
    "#layout = nx.shell_layout(graph)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "nx.draw_networkx_nodes(graph, layout, alpha = 0.7)\n",
    "nx.draw_networkx_labels(graph, layout, font_family='NanumBarunGothic', font_size = 14)\n",
    "nx.draw_networkx_edges(graph, layout, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoNFuUVgxhTp"
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph, str(datetime.now()).split()[0]+\"_\"+keyword+'.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5sVZPCYB22L"
   },
   "source": [
    "# 워드 클라우드 시각화\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnIFy4XaB7Kg"
   },
   "outputs": [],
   "source": [
    "corpus =  ' '.join(sum(data['keyword'],[]))\n",
    "wordcloud = WordCloud(font_path = 'NanumBarunGothic', width=1130, height=800, background_color='white').generate(corpus)\n",
    "\n",
    "# 워드 클라우드 이미지 생성 및 출력\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGMBg29RG-8W"
   },
   "outputs": [],
   "source": [
    "corpus =  ' '.join(sum(data['keyword_en'],[]))\n",
    "wordcloud = WordCloud(font_path = 'NanumBarunGothic', width=1130, height=800, background_color='white').generate(corpus)\n",
    "\n",
    "# 워드 클라우드 이미지 생성 및 출력\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSLR6gmLNRLE"
   },
   "outputs": [],
   "source": [
    "corpus =  ' '.join(sum(data['commentsKeyword'],[]))\n",
    "wordcloud = WordCloud(font_path = 'NanumBarunGothic', width=1130, height=800, background_color='white').generate(corpus)\n",
    "\n",
    "# 워드 클라우드 이미지 생성 및 출력\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZltbAFONUXk"
   },
   "outputs": [],
   "source": [
    "corpus =  ' '.join(sum(data['commentsKeyword_en'],[]))\n",
    "wordcloud = WordCloud(font_path = 'NanumBarunGothic', width=1130, height=800, background_color='white').generate(corpus)\n",
    "\n",
    "# 워드 클라우드 이미지 생성 및 출력\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsyCAUHmfk2i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "df_expanded = data.copy()\n",
    "df_expanded['Date'] = pd.to_datetime(df_expanded['Date'])\n",
    "\n",
    "\n",
    "# 키워드 리스트를 펼쳐서 각 키워드를 행 단위로 유지하고 날짜와 키워드를 열로 가지는 데이터프레임 생성\n",
    "df_expanded = df_expanded.explode('keyword')\n",
    "\n",
    "# 'date' 열을 인덱스로 설정\n",
    "df_expanded.set_index('Date', inplace=True)\n",
    "\n",
    "# 월별 키워드 빈도 계산\n",
    "monthly_counts = df_expanded.groupby([pd.Grouper(freq='M'), 'keyword']).size().unstack()\n",
    "\n",
    "# 꺾은선 그래프 그리기\n",
    "for i in ['']:# 원하는 키워드 선택\n",
    "  desired_keyword = i\n",
    "\n",
    "  # 원하는 키워드의 꺾은선 그래프 그리기\n",
    "  monthly_counts[desired_keyword].plot(kind='line', marker='o')\n",
    "\n",
    "# 그래프 스타일 및 레이블 설정\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Keyword Frequency by Month')\n",
    "plt.legend(title='Keyword')\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "62BlBTDsIBJb",
    "u5yvjLYPIRKR",
    "lENLB4No-Era",
    "w7BrP7MRxhTo",
    "P5sVZPCYB22L"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
